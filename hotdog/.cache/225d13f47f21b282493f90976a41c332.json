{"dependencies":[{"name":"/Users/sanghan/Desktop/hotdog/package.json","includedInParent":true,"mtime":1522646366212},{"name":"/Users/sanghan/Desktop/hotdog/node_modules/@tensorflow/tfjs-core/package.json","includedInParent":true,"mtime":0},{"name":"../doc","loc":{"line":9,"column":20}},{"name":"../globals","loc":{"line":10,"column":24}},{"name":"../util","loc":{"line":11,"column":19}},{"name":"./axis_util","loc":{"line":12,"column":24}},{"name":"./operation","loc":{"line":13,"column":26}},{"name":"./ops","loc":{"line":14,"column":18}}],"generated":{"js":"\"use strict\";\nvar __decorate = (this && this.__decorate) || function (decorators, target, key, desc) {\n    var c = arguments.length, r = c < 3 ? target : desc === null ? desc = Object.getOwnPropertyDescriptor(target, key) : desc, d;\n    if (typeof Reflect === \"object\" && typeof Reflect.decorate === \"function\") r = Reflect.decorate(decorators, target, key, desc);\n    else for (var i = decorators.length - 1; i >= 0; i--) if (d = decorators[i]) r = (c < 3 ? d(r) : c > 3 ? d(target, key, r) : d(target, key)) || r;\n    return c > 3 && r && Object.defineProperty(target, key, r), r;\n};\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar doc_1 = require(\"../doc\");\nvar globals_1 = require(\"../globals\");\nvar util = require(\"../util\");\nvar axis_util = require(\"./axis_util\");\nvar operation_1 = require(\"./operation\");\nvar ops = require(\"./ops\");\nvar SoftmaxOps = (function () {\n    function SoftmaxOps() {\n    }\n    SoftmaxOps.softmax = function (logits, dim) {\n        if (dim === void 0) { dim = -1; }\n        if (dim === -1) {\n            dim = logits.rank - 1;\n        }\n        if (dim !== logits.rank - 1) {\n            throw Error('Softmax along a non-last dimension is not yet supported. ' +\n                (\"Logits was rank \" + logits.rank + \" and dim was \" + dim));\n        }\n        var customOp = globals_1.customGrad(function (logits) {\n            var keepDims = true;\n            var lse = logits.logSumExp([dim], keepDims);\n            var logResult = logits.toFloat().sub(lse);\n            var y = logResult.exp();\n            var gradFunc = function (dy) {\n                var dyTimesY = dy.mul(y);\n                var keepDims = true;\n                return dyTimesY.sub(dyTimesY.sum([dim], keepDims).mul(y));\n            };\n            return { value: y, gradFunc: gradFunc };\n        });\n        return customOp(logits);\n    };\n    SoftmaxOps.softmaxCrossEntropy = function (labels, logits, dim) {\n        if (dim === void 0) { dim = -1; }\n        util.assertShapesMatch(labels.shape, logits.shape, 'Error in softmaxCrossEntropy: ');\n        if (dim === -1) {\n            dim = logits.rank - 1;\n        }\n        if (dim !== logits.rank - 1) {\n            throw Error(\"Softmax cross entropy along a non-last dimension is not yet \" +\n                (\"supported. Labels / logits was rank \" + logits.rank + \" \") +\n                (\"and dim was \" + dim));\n        }\n        var customOp = globals_1.customGrad(function (labels, logits) {\n            var predictedProbs = logits.softmax(dim);\n            var costVector = ops.scalar(1e-5).add(predictedProbs).log().mul(labels).neg();\n            var value = costVector.sum([dim]);\n            var gradFunc = function (dy) {\n                var dyShape = axis_util.expandShapeToKeepDim(dy.shape, [dim]);\n                return [\n                    dy.reshape(dyShape).mul(labels.toFloat().sub(predictedProbs)),\n                    dy.reshape(dyShape).mul(predictedProbs.sub(labels.toFloat())),\n                ];\n            };\n            return { value: value, gradFunc: gradFunc };\n        });\n        return customOp(labels, logits);\n    };\n    __decorate([\n        doc_1.doc({ heading: 'Operations', subheading: 'Normalization' }),\n        operation_1.operation\n    ], SoftmaxOps, \"softmax\", null);\n    __decorate([\n        doc_1.doc({ heading: 'Training', subheading: 'Losses', namespace: 'losses' }),\n        operation_1.operation\n    ], SoftmaxOps, \"softmaxCrossEntropy\", null);\n    return SoftmaxOps;\n}());\nexports.SoftmaxOps = SoftmaxOps;\n"},"hash":"c2dd0196ead2d3b8b5ed86b69184a56d","cacheData":{"env":{}}}