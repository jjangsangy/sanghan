{"dependencies":[{"name":"/Users/sanghan/Desktop/tfjs-examples/mobilenet/package.json","includedInParent":true,"mtime":1522644996454},{"name":"/Users/sanghan/Desktop/tfjs-examples/mobilenet/node_modules/@tensorflow/tfjs-layers/package.json","includedInParent":true,"mtime":1522260134000},{"name":"@tensorflow/tfjs-core","loc":{"line":13,"column":26}},{"name":"./backend/tfjs_backend","loc":{"line":14,"column":16}},{"name":"./errors","loc":{"line":15,"column":23}},{"name":"./utils/generic_utils","loc":{"line":16,"column":30}}],"generated":{"js":"\"use strict\";\nvar __extends = (this && this.__extends) || (function () {\n    var extendStatics = Object.setPrototypeOf ||\n        ({ __proto__: [] } instanceof Array && function (d, b) { d.__proto__ = b; }) ||\n        function (d, b) { for (var p in b) if (b.hasOwnProperty(p)) d[p] = b[p]; };\n    return function (d, b) {\n        extendStatics(d, b);\n        function __() { this.constructor = d; }\n        d.prototype = b === null ? Object.create(b) : (__.prototype = b.prototype, new __());\n    };\n})();\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar tfjs_core_1 = require(\"@tensorflow/tfjs-core\");\nvar K = require(\"./backend/tfjs_backend\");\nvar errors_1 = require(\"./errors\");\nvar generic_utils_1 = require(\"./utils/generic_utils\");\nvar LayersOptimizer = (function () {\n    function LayersOptimizer(config) {\n        if (config instanceof tfjs_core_1.Optimizer) {\n            this.createdFromCoreOptimizer = true;\n            this.constructFromCoreOptimizer(config);\n        }\n        else {\n            this.createdFromCoreOptimizer = false;\n            this.clipnorm = config.clipnorm;\n            this.clipvalue = config.clipvalue;\n            this.constructFromConfig(config);\n        }\n    }\n    LayersOptimizer.prototype.getConfig = function () {\n        if (this.createdFromCoreOptimizer) {\n            throw new errors_1.NotImplementedError('getConfig() for a LayersOptimizer constructed from a core ' +\n                'Optimizer is not supported yet.');\n        }\n        var config = {};\n        if (this.clipnorm != null) {\n            config['clipnorm'] = this.clipnorm;\n        }\n        if (this.clipvalue != null) {\n            config['clipvalue'] = this.clipvalue;\n        }\n        return config;\n    };\n    LayersOptimizer.prototype.updateVariables = function (lossFn, params) {\n        var variables = params.map(function (param) { return param.read(); });\n        return this.optimizer.minimize(lossFn, true, variables);\n    };\n    LayersOptimizer.fromConfig = function (cls, config) {\n        return new cls(config);\n    };\n    return LayersOptimizer;\n}());\nexports.LayersOptimizer = LayersOptimizer;\nvar SGD = (function (_super) {\n    __extends(SGD, _super);\n    function SGD(config) {\n        return _super.call(this, config) || this;\n    }\n    SGD.prototype.constructFromConfig = function (config) {\n        this.lr = (config.lr == null) ? 0.01 : config.lr;\n        if (this.lr < 0) {\n            throw new errors_1.ValueError(\"Invalid lr (\" + this.lr + \"). Must be >= 0 or undefined.\");\n        }\n        this.momentum = (config.momentum == null) ? 0.0 : config.momentum;\n        if (this.momentum < 0) {\n            throw new errors_1.ValueError(\"Invalid momentum (\" + this.momentum + \"). Must be >= 0 or undefined.\");\n        }\n        if (this.momentum !== 0) {\n            throw new errors_1.NotImplementedError('SGD momentum is not implemented yet.');\n        }\n        this.decay = (config.decay == null) ? 0.0 : config.decay;\n        if (this.decay < 0) {\n            throw new errors_1.ValueError(\"Invalid decay (\" + this.decay + \"). Must be >= 0 or undefined.\");\n        }\n        if (this.decay !== 0) {\n            throw new errors_1.NotImplementedError('SGD decay is not implemented yet');\n        }\n        this.nesterov = (config.nesterov == null) ? false : config.nesterov;\n        if (this.nesterov !== false) {\n            throw new errors_1.NotImplementedError('SGD nesterov is not implemented yet');\n        }\n        this.optimizer = tfjs_core_1.train.sgd(this.lr);\n    };\n    SGD.prototype.constructFromCoreOptimizer = function (optimizer) {\n        if (!(optimizer instanceof tfjs_core_1.SGDOptimizer)) {\n            throw new errors_1.ValueError('Cannot construct SGD from a non-SGD core optimizer');\n        }\n        this.optimizer = optimizer;\n    };\n    SGD.prototype.getConfig = function () {\n        var config = {\n            lr: this.lr,\n            momentum: this.momentum,\n            decay: this.decay,\n            nestorv: this.nesterov,\n        };\n        var baseConfig = _super.prototype.getConfig.call(this);\n        Object.assign(config, baseConfig);\n        return config;\n    };\n    return SGD;\n}(LayersOptimizer));\nexports.SGD = SGD;\ngeneric_utils_1.ClassNameMap.register('SGD', SGD);\nvar Adam = (function (_super) {\n    __extends(Adam, _super);\n    function Adam(config) {\n        return _super.call(this, config) || this;\n    }\n    Adam.prototype.constructFromConfig = function (config) {\n        this.lr = config.lr == null ? 0.001 : config.lr;\n        this.beta1 = config.beta_1 == null ? 0.9 : config.beta_1;\n        this.beta2 = config.beta_2 == null ? 0.999 : config.beta_2;\n        this.epsilon = config.epsilon == null ? K.epsilon() : config.epsilon;\n        this.decay = config.decay == null ? 0 : config.decay;\n        if (this.decay !== 0.0) {\n            throw new errors_1.NotImplementedError('Adam decay is not implemented yet');\n        }\n        this.amsgrad = config.amsgrad == null ? false : config.amsgrad;\n        if (this.amsgrad !== false) {\n            throw new errors_1.NotImplementedError('Adam amsgrad is not implemented yet');\n        }\n        this.optimizer = tfjs_core_1.train.adam(this.lr, this.beta1, this.beta2, this.epsilon);\n    };\n    Adam.prototype.constructFromCoreOptimizer = function (optimizer) {\n        if (!(optimizer instanceof tfjs_core_1.AdamOptimizer)) {\n            throw new errors_1.ValueError('Cannot construct Adam from a non-Adam core optimizer');\n        }\n        this.optimizer = optimizer;\n    };\n    Adam.prototype.getConfig = function () {\n        var config = {\n            lr: this.lr,\n            beta1: this.beta1,\n            beta2: this.beta2,\n            decay: this.decay,\n            epsilon: this.epsilon,\n            amsgrad: this.amsgrad\n        };\n        var baseConfig = _super.prototype.getConfig.call(this);\n        Object.assign(config, baseConfig);\n        return config;\n    };\n    return Adam;\n}(LayersOptimizer));\nexports.Adam = Adam;\ngeneric_utils_1.ClassNameMap.register('Adam', Adam);\nvar RMSProp = (function (_super) {\n    __extends(RMSProp, _super);\n    function RMSProp(config) {\n        return _super.call(this, config) || this;\n    }\n    RMSProp.prototype.constructFromConfig = function (config) {\n        this.lr = config.lr == null ? 0.001 : config.lr;\n        this.rho = config.rho == null ? 0.9 : config.rho;\n        this.epsilon = config.epsilon == null ? K.epsilon() : config.epsilon;\n        if (config.decay != null) {\n            throw new errors_1.NotImplementedError('RMSProp decay is not implemented yet');\n        }\n        this.optimizer = tfjs_core_1.train.rmsprop(this.lr, this.rho, null, this.epsilon);\n    };\n    RMSProp.prototype.constructFromCoreOptimizer = function (optimizer) {\n        if (!(optimizer instanceof tfjs_core_1.RMSPropOptimizer)) {\n            throw new errors_1.ValueError('Cannot construct RMSProp from a non-RMSProp core optimizer');\n        }\n        this.optimizer = optimizer;\n    };\n    RMSProp.prototype.getConfig = function () {\n        var config = {\n            lr: this.lr,\n            rho: this.rho,\n            decay: this.decay,\n            epsilon: this.epsilon,\n        };\n        var baseConfig = _super.prototype.getConfig.call(this);\n        Object.assign(config, baseConfig);\n        return config;\n    };\n    return RMSProp;\n}(LayersOptimizer));\nexports.RMSProp = RMSProp;\ngeneric_utils_1.ClassNameMap.register('RMSProp', RMSProp);\nvar Adagrad = (function (_super) {\n    __extends(Adagrad, _super);\n    function Adagrad(config) {\n        return _super.call(this, config) || this;\n    }\n    Adagrad.prototype.constructFromConfig = function (config) {\n        this.lr = config.lr == null ? 0.01 : config.lr;\n        this.epsilon = config.epsilon == null ? K.epsilon() : config.epsilon;\n        this.decay = config.decay == null ? 0 : config.decay;\n        if (this.decay !== 0) {\n            throw new errors_1.NotImplementedError('Adagrad decay is not implemented yet');\n        }\n        this.optimizer = tfjs_core_1.train.adagrad(this.lr);\n    };\n    Adagrad.prototype.constructFromCoreOptimizer = function (optimizer) {\n        if (!(optimizer instanceof tfjs_core_1.AdagradOptimizer)) {\n            throw new errors_1.ValueError('Cannot construct Adagrad from a non-Adagrad core optimizer');\n        }\n        this.optimizer = optimizer;\n    };\n    Adagrad.prototype.getConfig = function () {\n        var config = {\n            lr: this.lr,\n            decay: this.decay,\n            epsilon: this.epsilon,\n        };\n        var baseConfig = _super.prototype.getConfig.call(this);\n        Object.assign(config, baseConfig);\n        return config;\n    };\n    return Adagrad;\n}(LayersOptimizer));\nexports.Adagrad = Adagrad;\ngeneric_utils_1.ClassNameMap.register('Adagrad', Adagrad);\nexports.adagrad = Adagrad;\nexports.adam = Adam;\nexports.rmsprop = RMSProp;\nexports.sgd = SGD;\nfunction get(identifier) {\n    var coreOptimizerToConstructorMap = {\n        'AdagradOptimizer': Adagrad,\n        'AdamOptimizer': Adam,\n        'RMSPropOptimizer': RMSProp,\n        'SGDOptimizer': SGD\n    };\n    var optimizerMap = { Adagrad: Adagrad, Adam: Adam, RMSProp: RMSProp, SGD: SGD, adagrad: exports.adagrad, adam: exports.adam, rmsprop: exports.rmsprop, sgd: exports.sgd };\n    if (typeof identifier === 'string') {\n        if (identifier in optimizerMap) {\n            return optimizerMap[identifier];\n        }\n        throw new errors_1.ValueError(\"Unknown Optimizer \" + identifier);\n    }\n    else {\n        var coreOptimizerTypeName = identifier.constructor.name;\n        if (coreOptimizerTypeName in coreOptimizerToConstructorMap) {\n            return coreOptimizerToConstructorMap[coreOptimizerTypeName];\n        }\n        throw new errors_1.ValueError(\"Unsupported core optimizer type: \" + coreOptimizerTypeName);\n    }\n}\nexports.get = get;\n"},"hash":"da2e832270666e640ce7b565630ca04a","cacheData":{"env":{}}}