{"dependencies":[{"name":"/Users/sanghan/Desktop/tfjs-examples/mobilenet/package.json","includedInParent":true,"mtime":1522644996454},{"name":"/Users/sanghan/Desktop/tfjs-examples/mobilenet/node_modules/@tensorflow/tfjs-layers/package.json","includedInParent":true,"mtime":1522260134000},{"name":"@tensorflow/tfjs-core","loc":{"line":4,"column":26}},{"name":"underscore","loc":{"line":5,"column":16}},{"name":"../common","loc":{"line":6,"column":23}},{"name":"../errors","loc":{"line":7,"column":23}},{"name":"../types","loc":{"line":8,"column":22}},{"name":"../utils/generic_utils","loc":{"line":9,"column":30}},{"name":"../utils/math_utils","loc":{"line":10,"column":25}},{"name":"./common","loc":{"line":12,"column":23}}],"generated":{"js":"\"use strict\";\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar tfc = require(\"@tensorflow/tfjs-core\");\nvar tfjs_core_1 = require(\"@tensorflow/tfjs-core\");\nvar _ = require(\"underscore\");\nvar common_1 = require(\"../common\");\nvar errors_1 = require(\"../errors\");\nvar types_1 = require(\"../types\");\nvar generic_utils_1 = require(\"../utils/generic_utils\");\nvar math_utils = require(\"../utils/math_utils\");\nvar common_2 = require(\"./common\");\nvar common_3 = require(\"./common\");\nvar backend = 'webgl';\nvar DEFAULT_DTYPE = types_1.DType.float32;\nfunction disposeScalarCache() {\n    for (var typeKey in scalarCache) {\n        for (var key in scalarCache[typeKey]) {\n            scalarCache[typeKey][key].dispose();\n            delete scalarCache[typeKey][key];\n        }\n    }\n}\nfunction setBackend(requestedBackend) {\n    tfc.setBackend(requestedBackend);\n    backend = requestedBackend;\n    disposeScalarCache();\n}\nexports.setBackend = setBackend;\nfunction getBackend() {\n    return backend;\n}\nexports.getBackend = getBackend;\nfunction keep(x) {\n    return tfc.keep(x);\n}\nexports.keep = keep;\nvar scalarCache = {\n    float32: {},\n    int32: {}\n};\nfunction getScalar(value, dtype) {\n    if (dtype === undefined) {\n        dtype = DEFAULT_DTYPE;\n    }\n    if (scalarCache[dtype][value] == null) {\n        scalarCache[dtype][value] = tfjs_core_1.scalar(value, dtype);\n        tfc.keep(scalarCache[dtype][value]);\n    }\n    return scalarCache[dtype][value];\n}\nexports.getScalar = getScalar;\nexports.epsilon = common_2.epsilon;\nfunction isBackendSymbolic() {\n    return false;\n}\nexports.isBackendSymbolic = isBackendSymbolic;\nfunction shape(x) {\n    return x.shape;\n}\nexports.shape = shape;\nfunction intShape(x) {\n    return x.shape;\n}\nexports.intShape = intShape;\nfunction ndim(x) {\n    return x.shape.length;\n}\nexports.ndim = ndim;\nfunction dtype(x) {\n    return (x instanceof tfjs_core_1.Tensor) ? DEFAULT_DTYPE : x.dtype;\n}\nexports.dtype = dtype;\nfunction normalizeAxis(x, axis) {\n    if (axis == null) {\n        return axis;\n    }\n    var xShape = shape(x);\n    if (Array.isArray(axis)) {\n        return axis.map(function (thisAxis) { return generic_utils_1.pyNormalizeArrayIndex(xShape, thisAxis); });\n    }\n    return generic_utils_1.pyNormalizeArrayIndex(xShape, axis);\n}\nexports.normalizeAxis = normalizeAxis;\nfunction countParams(x) {\n    var shape = x.shape;\n    if (shape.length > 0) {\n        return shape.reduce(function (a, b) { return a * b; });\n    }\n    else {\n        return 1;\n    }\n}\nexports.countParams = countParams;\nfunction cast(x, dtype) {\n    return x.asType(dtype);\n}\nexports.cast = cast;\nfunction reshape(x, shape) {\n    return x.reshape(shape);\n}\nexports.reshape = reshape;\nfunction transpose(x, perm) {\n    return tfc.transpose(x, perm);\n}\nexports.transpose = transpose;\nexports.permuteDimensions = transpose;\nfunction reverse(x, axes) {\n    return tfc.reverse(x, axes);\n}\nexports.reverse = reverse;\nfunction expandDims(x, axis) {\n    if (axis === void 0) { axis = -1; }\n    var outShape = shape(x).slice();\n    if (axis < 0) {\n        axis = outShape.length + axis + 1;\n    }\n    outShape.splice(axis, 0, 1);\n    return reshape(x, outShape);\n}\nexports.expandDims = expandDims;\nfunction squeeze(x, axis) {\n    return tfc.squeeze(x, [axis]);\n}\nexports.squeeze = squeeze;\nfunction repeat(x, n) {\n    if (x.shape.length !== 2) {\n        throw new errors_1.ValueError(\"repeat() expects a rank-2 tensor, but received a \" +\n            (\"rank-\" + x.shape.length + \" tensor.\"));\n    }\n    var y = expandDims(x, 1);\n    return tile(y, [1, n, 1]);\n}\nexports.repeat = repeat;\nfunction flatten(x) {\n    var newShape = [math_utils.arrayProd(x.shape)];\n    return reshape(x, newShape);\n}\nexports.flatten = flatten;\nfunction batchFlatten(x) {\n    if (ndim(x) <= 1) {\n        throw new errors_1.ValueError(\"batchFlatten requires a minimum rank of 2. Got rank: \" + ndim(x) + \".\");\n    }\n    var newShape = [x.shape[0], math_utils.arrayProd(x.shape, 1)];\n    return reshape(x, newShape);\n}\nexports.batchFlatten = batchFlatten;\nfunction sliceAlongFirstAxis(array, start, size) {\n    switch (array.rank) {\n        case 1:\n            return tfc.slice1d(array, start, size);\n        case 2:\n            return tfc.slice2d(array, [start, 0], [size, array.shape[1]]);\n        case 3:\n            return tfc.slice3d(array, [start, 0, 0], [size, array.shape[1], array.shape[2]]);\n        case 4:\n            return tfc.slice4d(array, [start, 0, 0, 0], [size, array.shape[1], array.shape[2], array.shape[3]]);\n        default:\n            throw new errors_1.ValueError(\"sliceAlongFirstAxis() received an unsupported subtype of Tensor: \" +\n                (\"\" + array.constructor.name));\n    }\n}\nexports.sliceAlongFirstAxis = sliceAlongFirstAxis;\nfunction sliceAlongLastAxis(array, start, size) {\n    switch (array.rank) {\n        case 1:\n            return tfc.slice1d(array, start, size);\n        case 2:\n            return tfc.slice2d(array, [0, start], [array.shape[0], size]);\n        case 3:\n            return tfc.slice3d(array, [0, 0, start], [array.shape[0], array.shape[1], size]);\n        case 4:\n            return tfc.slice4d(array, [0, 0, 0, start], [array.shape[0], array.shape[1], array.shape[2], size]);\n        default:\n            throw new errors_1.ValueError(\"sliceAlongLastAxis() received an unsupported subtype of Tensor: \" +\n                (\"\" + array.constructor.name));\n    }\n}\nexports.sliceAlongLastAxis = sliceAlongLastAxis;\nfunction concatenate(tensors, axis) {\n    if (axis === void 0) { axis = -1; }\n    var rank;\n    if (axis < 0) {\n        rank = ndim(tensors[0]);\n        if (rank !== 0) {\n            axis = rank;\n        }\n        else {\n            axis = 0;\n        }\n    }\n    if (axis === ndim(tensors[0])) {\n        axis = -1;\n    }\n    return tfc.concat(tensors, axis);\n}\nexports.concatenate = concatenate;\nfunction concatAlongFirstAxis(a, b) {\n    switch (a.rank) {\n        case 1:\n            return tfc.concat1d([a, b]);\n        case 2:\n            return tfc.concat2d([a, b], 0);\n        case 3:\n            return tfc.concat3d([a, b], 0);\n        case 4:\n            return tfc.concat4d([a, b], 0);\n        default:\n            throw new errors_1.ValueError('concatAlongFirstAxis() received an unsupported subtype of ' +\n                'Tensor: ' + a.constructor.name);\n    }\n}\nexports.concatAlongFirstAxis = concatAlongFirstAxis;\nfunction tile(x, n) {\n    if (!Array.isArray(n)) {\n        n = [n];\n    }\n    if (ndim(x) !== n.length) {\n        throw new errors_1.ValueError(\"The length of input n (\" + n.length + \") does not match \" +\n            (\"the number of dimensions in input x (\" + ndim(x) + \")\"));\n    }\n    return tfc.tile(x, n);\n}\nexports.tile = tile;\nfunction variable(x, dtype, name, constraint) {\n    return new types_1.LayerVariable(x, dtype, name, true, constraint);\n}\nexports.variable = variable;\nfunction batchGetValue(xs) {\n    return xs.map(function (x) { return x.read(); });\n}\nexports.batchGetValue = batchGetValue;\nfunction batchSetValue(variablesAndValues) {\n    variablesAndValues.map(function (variableAndValue) {\n        var variable = variableAndValue[0];\n        variable.write(variableAndValue[1]);\n    });\n}\nexports.batchSetValue = batchSetValue;\nfunction zeros(shape, dtype) {\n    return tfc.zeros(shape);\n}\nexports.zeros = zeros;\nfunction zerosVariable(shape, dtype, name) {\n    return new types_1.LayerVariable(zeros(shape), dtype, name);\n}\nexports.zerosVariable = zerosVariable;\nfunction zerosLike(x, dtype, name) {\n    return new types_1.LayerVariable(tfc.zerosLike(x), dtype, name);\n}\nexports.zerosLike = zerosLike;\nfunction ones(shape, dtype) {\n    return tfc.ones(shape);\n}\nexports.ones = ones;\nfunction onesVariable(shape, dtype, name) {\n    var allocated = tfc.ones(shape);\n    return new types_1.LayerVariable(allocated, dtype, name);\n}\nexports.onesVariable = onesVariable;\nfunction onesLike(x, dtype, name) {\n    var allocated = tfc.onesLike(x);\n    return new types_1.LayerVariable(allocated, dtype, name);\n}\nexports.onesLike = onesLike;\nfunction identity(x) {\n    return x.clone();\n}\nexports.identity = identity;\nfunction eye(size, dtype, name) {\n    var buffer = [];\n    for (var i = 0; i < size; ++i) {\n        for (var j = 0; j < size; ++j) {\n            buffer.push(i === j ? 1 : 0);\n        }\n    }\n    return tfjs_core_1.tensor2d(buffer, [size, size]);\n}\nexports.eye = eye;\nfunction eyeVariable(size, dtype, name) {\n    return new types_1.LayerVariable(eye(size, dtype), dtype, name);\n}\nexports.eyeVariable = eyeVariable;\nfunction neg(x) {\n    return tfc.neg(x);\n}\nexports.neg = neg;\nfunction add(x, y) {\n    return tfc.add(x, y);\n}\nexports.add = add;\nfunction subtract(x, y) {\n    return tfc.sub(x, y);\n}\nexports.subtract = subtract;\nfunction multiply(x, y) {\n    return tfc.mul(x, y);\n}\nexports.multiply = multiply;\nfunction divide(x, y) {\n    return tfc.div(x, y);\n}\nexports.divide = divide;\nfunction scalarTimesArray(c, x) {\n    return tfc.mul(c, x);\n}\nexports.scalarTimesArray = scalarTimesArray;\nfunction scalarPlusArray(c, x) {\n    return tfc.add(c, x);\n}\nexports.scalarPlusArray = scalarPlusArray;\nfunction randomUniform(shape, minval, maxval, dtype, seed) {\n    return tfc.randomUniform(shape, minval, maxval);\n}\nexports.randomUniform = randomUniform;\nfunction randomUniformVariable(shape, minval, maxval, dtype, seed, name) {\n    if (name === void 0) { name = 'randomUniform'; }\n    return new types_1.LayerVariable(randomUniform(shape, minval, maxval, dtype, seed), dtype, name);\n}\nexports.randomUniformVariable = randomUniformVariable;\nfunction truncatedNormal(shape, mean, stddev, dtype, seed) {\n    if (mean === void 0) { mean = 0.0; }\n    if (stddev === void 0) { stddev = 1.0; }\n    return tfc.truncatedNormal(shape, mean, stddev);\n}\nexports.truncatedNormal = truncatedNormal;\nfunction truncatedNormalVariable(shape, mean, stddev, dtype, seed, name) {\n    if (mean === void 0) { mean = 0.0; }\n    if (stddev === void 0) { stddev = 1.0; }\n    if (name === void 0) { name = 'truncatedNormal'; }\n    return new types_1.LayerVariable(truncatedNormal(shape, mean, stddev, dtype, seed), dtype, name);\n}\nexports.truncatedNormalVariable = truncatedNormalVariable;\nfunction randomNormal(shape, mean, stddev, dtype, seed) {\n    if (mean === void 0) { mean = 0.0; }\n    if (stddev === void 0) { stddev = 1.0; }\n    if (dtype === types_1.DType.bool) {\n        throw new errors_1.NotImplementedError(\"randomNormal does not support dType bool.\");\n    }\n    var dtypeString = (dtype === types_1.DType.float32) ? 'float32' : 'int32';\n    return tfc.randomNormal(shape, mean, stddev, dtypeString, seed);\n}\nexports.randomNormal = randomNormal;\nfunction randomNormalVariable(shape, mean, stddev, dtype, seed, name) {\n    if (mean === void 0) { mean = 0.0; }\n    if (stddev === void 0) { stddev = 1.0; }\n    if (name === void 0) { name = 'randomNormal'; }\n    return new types_1.LayerVariable(randomNormal(shape, mean, stddev, dtype, seed), dtype, name);\n}\nexports.randomNormalVariable = randomNormalVariable;\nfunction update(x, xNew) {\n    return x.write(xNew);\n}\nexports.update = update;\nfunction updateAdd(x, increment) {\n    return x.write(tfc.add(x.read(), increment));\n}\nexports.updateAdd = updateAdd;\nfunction updateSub(x, decrement) {\n    return x.write(tfc.sub(x.read(), decrement));\n}\nexports.updateSub = updateSub;\nfunction dot(x, y) {\n    if (ndim(y) !== 2) {\n        throw new errors_1.NotImplementedError(\"dot support for y other than rank 2 is not yet implemented: \" +\n            (\"y shape = \" + shape));\n    }\n    else {\n        if (ndim(x) === 2) {\n            return tfc.matMul(x, y);\n        }\n        else if (ndim(x) === 3) {\n            var xShape0 = x.shape[0];\n            var xShape1 = x.shape[1];\n            var xShape2 = x.shape[2];\n            x = x.reshape([xShape0 * xShape1, xShape2]);\n            return tfc.matMul(x, y).reshape([\n                xShape0, xShape1, y.shape[1]\n            ]);\n        }\n        else {\n            throw new errors_1.NotImplementedError(\"dot support for x of rank \" + ndim(x) + \" is not yet implemented: \" +\n                (\"x shape = \" + shape));\n        }\n    }\n}\nexports.dot = dot;\nfunction sign(x) {\n    var zerosLikeX = tfjs_core_1.zerosLike(x);\n    var onesLikeX = tfjs_core_1.onesLike(x);\n    return tfjs_core_1.where(equal(x, zerosLikeX), zerosLikeX, tfjs_core_1.where(greater(x, tfjs_core_1.zerosLike(x)), onesLikeX, scalarTimesArray(getScalar(-1), onesLikeX)));\n}\nexports.sign = sign;\nfunction qr(x) {\n    if (x.shape.length !== 2) {\n        throw new errors_1.ValueError(\"qr() requires a 2D Tensor, but got a \" + x.shape.length + \"D Tensor.\");\n    }\n    if (x.shape[0] < x.shape[1]) {\n        throw new errors_1.ValueError(\"qr() requires x.shape[0] >= x.shape[1], but got shape: [\" + x.shape + \"]\");\n    }\n    var m = x.shape[0];\n    var n = x.shape[1];\n    var q = eye(m);\n    var r = x;\n    var one2D = tfjs_core_1.tensor2d([[1]], [1, 1]);\n    for (var j = 0; j < n; ++j) {\n        var rjEnd1 = r.slice([j, j], [m - j, 1]);\n        var normX = tfc.norm(rjEnd1);\n        var rjj = r.slice([j, j], [1, 1]);\n        var s = tfc.neg(sign(rjj));\n        var u1 = rjj.sub(multiply(s, normX));\n        var wPre = divide(rjEnd1, u1);\n        var w = void 0;\n        if (wPre.shape[0] === 1) {\n            w = one2D;\n        }\n        else {\n            w = one2D.concat(wPre.slice([1, 0], [wPre.shape[0] - 1, wPre.shape[1]]), 0);\n        }\n        var tau = tfc.neg(divide(tfc.matMul(s, u1), normX));\n        var rjEndAll = r.slice([j, 0], [m - j, n]);\n        var tauTimesW = tau.mul(w);\n        if (j === 0) {\n            r = rjEndAll.sub(tauTimesW.matMul(w.transpose().matMul(rjEndAll)));\n        }\n        else {\n            r = r.slice([0, 0], [j, n])\n                .concat(rjEndAll.sub(tauTimesW.matMul(w.transpose().matMul(rjEndAll))), 0);\n        }\n        var qAllJEnd = q.slice([0, j], [m, q.shape[1] - j]);\n        if (j === 0) {\n            q = qAllJEnd.sub(qAllJEnd.matMul(w).matMul(tauTimesW.transpose()));\n        }\n        else {\n            q = q.slice([0, 0], [m, j])\n                .concat(qAllJEnd.sub(qAllJEnd.matMul(w).matMul(tauTimesW.transpose())), 1);\n        }\n    }\n    return [q, r];\n}\nexports.qr = qr;\nfunction oneHot(indices, numClasses) {\n    if (ndim(indices) !== 1) {\n        throw new Error('Only 1D one-hot tensors are supported in the ' +\n            'deeplearn backend, at present.');\n    }\n    return tfc.oneHot(indices, numClasses);\n}\nexports.oneHot = oneHot;\nfunction mean(x, axis, keepDims) {\n    axis = normalizeAxis(x, axis);\n    return tfc.mean(x, axis, keepDims);\n}\nexports.mean = mean;\nfunction argmax(x, axis) {\n    if (axis === void 0) { axis = -1; }\n    return tfc.argMax(x, axis);\n}\nexports.argmax = argmax;\nfunction gather(reference, indices, axis) {\n    if (Array.isArray(indices)) {\n        indices = tfjs_core_1.tensor1d(indices);\n    }\n    return tfc.gather(reference, indices, axis);\n}\nexports.gather = gather;\nfunction max(x, axis, keepDims) {\n    return tfc.max(x, axis, keepDims);\n}\nexports.max = max;\nfunction min(x, axis, keepDims) {\n    return tfc.min(x, axis, keepDims);\n}\nexports.min = min;\nfunction minimum(x, y) {\n    return tfc.minimum(x, y);\n}\nexports.minimum = minimum;\nfunction sum(x, axis, keepDims) {\n    return tfc.sum(x, axis, keepDims);\n}\nexports.sum = sum;\nfunction abs(x) {\n    return tfc.abs(x);\n}\nexports.abs = abs;\nfunction square(x) {\n    return tfc.mulStrict(x, x);\n}\nexports.square = square;\nfunction sqrt(x) {\n    return tfc.sqrt(x);\n}\nexports.sqrt = sqrt;\nfunction exp(x) {\n    return tfc.exp(x);\n}\nexports.exp = exp;\nfunction log(x) {\n    return tfc.log(x);\n}\nexports.log = log;\nfunction pow(x, a) {\n    if (typeof (a) === 'number') {\n        a = tfjs_core_1.scalar(Math.round(a), 'int32');\n    }\n    if (a.dtype !== 'int32') {\n        throw new errors_1.NotImplementedError(\"Non-int32 dtype (\" + a.dtype + \") is not supported by pow() yet\");\n    }\n    return tfc.pow(x, a);\n}\nexports.pow = pow;\nfunction clip(x, minValue, maxValue) {\n    return tfc.clipByValue(x, minValue, maxValue);\n}\nexports.clip = clip;\nfunction equal(x, y) {\n    return tfc.equal(x, y);\n}\nexports.equal = equal;\nfunction greater(x, y) {\n    return tfc.greater(x, y);\n}\nexports.greater = greater;\nfunction greaterEqual(x, y) {\n    return tfc.greaterEqual(x, y);\n}\nexports.greaterEqual = greaterEqual;\nfunction maximum(x, y) {\n    return tfc.maximum(x, y);\n}\nexports.maximum = maximum;\nfunction sin(x) {\n    return tfc.sin(x.value());\n}\nexports.sin = sin;\nfunction cos(x) {\n    return tfc.cos(x.value());\n}\nexports.cos = cos;\nfunction batchNormalization(x, mean, variance, beta, gamma, epsilon) {\n    if (epsilon === void 0) { epsilon = 1e-3; }\n    var out;\n    if (ndim(x) === 2) {\n        out = tfc.batchNormalization2d(x, mean, variance, epsilon);\n    }\n    else if (ndim(x) === 3) {\n        out = tfc.batchNormalization3d(x, mean, variance, epsilon);\n    }\n    else if (ndim(x) === 4) {\n        out = tfc.batchNormalization4d(x, mean, variance, epsilon);\n    }\n    else {\n        throw new errors_1.NotImplementedError(\"batchNormalization is not implememnted for array of rank \" + ndim(x) + \" \" +\n            \"yet\");\n    }\n    if (gamma != null) {\n        out = multiply(out, gamma);\n    }\n    if (beta != null) {\n        out = add(out, beta);\n    }\n    return out;\n}\nexports.batchNormalization = batchNormalization;\nfunction biasAdd(x, bias, dataFormat) {\n    common_1.checkDataFormat(dataFormat);\n    if (ndim(bias) !== 1 && ndim(bias) !== ndim(x)) {\n        throw new errors_1.ValueError('Unexpected bias dimensions: ' + ndim(bias) +\n            '; expected it to be 1 or ' + ndim(x));\n    }\n    if (dataFormat) {\n        throw new errors_1.NotImplementedError('dataFormat logic is not yet implemented.');\n    }\n    return tfc.add(x, bias);\n}\nexports.biasAdd = biasAdd;\nfunction elu(x, alpha) {\n    if (alpha === void 0) { alpha = 1; }\n    if (alpha !== 1) {\n        throw new errors_1.NotImplementedError(\"Support for alpha values other than 1 (\" + alpha + \") is not implemented \" +\n            \"yet.\");\n    }\n    return tfc.elu(x);\n}\nexports.elu = elu;\nfunction selu(x) {\n    return tfc.selu(x);\n}\nexports.selu = selu;\nfunction relu(x) {\n    return tfc.relu(x);\n}\nexports.relu = relu;\nfunction softplus(x) {\n    return tfc.log(tfc.add(getScalar(1), tfc.exp(x)));\n}\nexports.softplus = softplus;\nfunction softsign(x) {\n    return tfc.div(x, tfc.add(getScalar(1), tfc.abs(x)));\n}\nexports.softsign = softsign;\nfunction tanh(x) {\n    return tfc.tanh(x);\n}\nexports.tanh = tanh;\nfunction dropout(x, level, noiseShape, seed) {\n    if (noiseShape != null && !_.isEqual(x.shape, noiseShape)) {\n        throw new errors_1.NotImplementedError('Non-default noise shape is not implemented yet: ' +\n            JSON.stringify(noiseShape));\n    }\n    if (seed != null) {\n        throw new errors_1.NotImplementedError('seed is not implemented for dropout yet.');\n    }\n    var multiplier = tfc.step(tfc.add(neg(level), randomUniform(x.shape, 0, 1, types_1.DType.float32)));\n    multiplier = tfc.mul(divide(getScalar(1), subtract(getScalar(1), level)), multiplier);\n    return tfc.mul(x, multiplier);\n}\nexports.dropout = dropout;\nfunction l2Normalize(x, axis) {\n    var squareSum = sum(square(x), axis, true);\n    var epsilonTensor = scalarTimesArray(tfjs_core_1.scalar(exports.epsilon()), tfc.onesLike(x));\n    var norm = sqrt(maximum(squareSum, epsilonTensor));\n    return divide(x, norm);\n}\nexports.l2Normalize = l2Normalize;\nfunction preprocessConv2DInput(x, dataFormat) {\n    common_1.checkDataFormat(dataFormat);\n    if (dataFormat === 'channelsFirst') {\n        return tfc.transpose(x, [0, 2, 3, 1]);\n    }\n    else {\n        return x;\n    }\n}\nfunction conv1dWithBias(x, kernel, bias, strides, padding, dataFormat, dilationRate) {\n    if (strides === void 0) { strides = 1; }\n    if (padding === void 0) { padding = 'valid'; }\n    if (dilationRate === void 0) { dilationRate = 1; }\n    if (dataFormat == null) {\n        dataFormat = common_3.imageDataFormat();\n    }\n    common_1.checkDataFormat(dataFormat);\n    if (dilationRate !== 1) {\n        throw new errors_1.NotImplementedError(\"dilationRate = \" + dilationRate + \" is not implemented for 1D \" +\n            \"convolution yet.\");\n    }\n    if (x.shape.length !== 3) {\n        throw new errors_1.ValueError(\"The input of a conv1dWithBias operation should be 3, but is \" +\n            (x.shape.length + \" instead.\"));\n    }\n    if (kernel.shape.length !== 3) {\n        throw new errors_1.ValueError(\"The kernel for a conv1dWithBias operation should be 3, but is \" +\n            (kernel.shape.length + \" instead\"));\n    }\n    if (bias != null && bias.shape.length !== 1) {\n        throw new errors_1.ValueError(\"The bias for a conv1dWithBias operation should be 1, but is \" +\n            (kernel.shape.length + \" instead\"));\n    }\n    if (dataFormat === 'channelsFirst') {\n        x = transpose(x, [0, 2, 1]);\n    }\n    if (padding === 'casual') {\n        throw new errors_1.NotImplementedError('The support for CASUAL padding mode in conv1dWithBias is not ' +\n            'implemented yet.');\n    }\n    var y = tfc.conv1d(x, kernel, strides, padding === 'same' ? 'same' : 'valid');\n    if (bias != null) {\n        y = biasAdd(y, bias);\n    }\n    return y;\n}\nexports.conv1dWithBias = conv1dWithBias;\nfunction conv1d(x, kernel, strides, padding, dataFormat, dilationRate) {\n    if (strides === void 0) { strides = 1; }\n    if (padding === void 0) { padding = 'valid'; }\n    if (dilationRate === void 0) { dilationRate = 1; }\n    common_1.checkDataFormat(dataFormat);\n    return conv1dWithBias(x, kernel, null, strides, padding, dataFormat, dilationRate);\n}\nexports.conv1d = conv1d;\nfunction conv2d(x, kernel, strides, padding, dataFormat, dilationRate) {\n    if (strides === void 0) { strides = [1, 1]; }\n    if (padding === void 0) { padding = 'valid'; }\n    common_1.checkDataFormat(dataFormat);\n    return conv2dWithBias(x, kernel, null, strides, padding, dataFormat, dilationRate);\n}\nexports.conv2d = conv2d;\nfunction conv2dWithBias(x, kernel, bias, strides, padding, dataFormat, dilationRate) {\n    if (strides === void 0) { strides = [1, 1]; }\n    if (padding === void 0) { padding = 'valid'; }\n    if (dataFormat == null) {\n        dataFormat = common_3.imageDataFormat();\n    }\n    common_1.checkDataFormat(dataFormat);\n    if (dilationRate != null) {\n        throw new errors_1.NotImplementedError('Support for non-default dilation rate is not implemented yet.');\n    }\n    if (ndim(x) !== 3 && ndim(x) !== 4) {\n        throw new errors_1.ValueError(\"conv2dWithBias expects input to be of rank 3 or 4, but received \" +\n            (ndim(x) + \".\"));\n    }\n    if (ndim(kernel) !== 3 && ndim(kernel) !== 4) {\n        throw new errors_1.ValueError(\"conv2dWithBias expects kernel to be of rank 3 or 4, but received \" +\n            (ndim(x) + \".\"));\n    }\n    var y = preprocessConv2DInput(x, dataFormat);\n    if (padding === 'casual') {\n        throw new errors_1.NotImplementedError('The support for CASUAL padding mode in conv1dWithBias is not ' +\n            'implemented yet.');\n    }\n    y = tfc.conv2d(y, kernel, strides, padding === 'same' ? 'same' : 'valid');\n    if (bias != null) {\n        y = biasAdd(y, bias);\n    }\n    if (dataFormat === 'channelsFirst') {\n        y = tfc.transpose(y, [0, 3, 1, 2]);\n    }\n    return y;\n}\nexports.conv2dWithBias = conv2dWithBias;\nfunction depthwiseConv2d(x, depthwiseKernel, strides, padding, dataFormat, dilationRate) {\n    if (strides === void 0) { strides = [1, 1]; }\n    if (padding === void 0) { padding = 'valid'; }\n    if (dataFormat == null) {\n        dataFormat = common_3.imageDataFormat();\n    }\n    common_1.checkDataFormat(dataFormat);\n    var y = preprocessConv2DInput(x, dataFormat);\n    if (ndim(x) !== 4) {\n        throw new errors_1.ValueError(\"Input for depthwiseConv2d is required to be 4-D, but is instead \" +\n            (ndim(x) + \"-D\"));\n    }\n    if (ndim(depthwiseKernel) !== 4) {\n        throw new errors_1.ValueError(\"depthwiseKernel is required to be 4-D, but is instead \" +\n            (ndim(depthwiseKernel) + \"-D\"));\n    }\n    y = tfc.depthwiseConv2d(y, depthwiseKernel, strides, padding === 'same' ? 'same' : 'valid', 'NHWC', dilationRate);\n    if (dataFormat === 'channelsFirst') {\n        y = tfc.transpose(y, [0, 3, 1, 2]);\n    }\n    return y;\n}\nexports.depthwiseConv2d = depthwiseConv2d;\nfunction pool2d(x, poolSize, strides, padding, dataFormat, poolMode) {\n    common_1.checkDataFormat(dataFormat);\n    common_1.checkPoolMode(poolMode);\n    common_1.checkPaddingMode(padding);\n    if (strides == null) {\n        strides = [1, 1];\n    }\n    if (padding == null) {\n        padding = 'valid';\n    }\n    if (dataFormat == null) {\n        dataFormat = common_3.imageDataFormat();\n    }\n    if (poolMode == null) {\n        poolMode = 'max';\n    }\n    x = preprocessConv2DInput(x, dataFormat);\n    var y;\n    var paddingString = (padding === 'same') ? 'same' : 'valid';\n    if (poolMode === 'max') {\n        y = tfc.maxPool(x, poolSize, strides, paddingString);\n    }\n    else {\n        y = tfc.avgPool(x, poolSize, strides, paddingString);\n    }\n    if (dataFormat === 'channelsFirst') {\n        y = tfc.transpose(y, [0, 3, 1, 2]);\n    }\n    return y;\n}\nexports.pool2d = pool2d;\nfunction nameScope(name, fn) {\n    return common_1.nameScope(name, fn);\n}\nexports.nameScope = nameScope;\nfunction floatx() {\n    return types_1.DType.float32;\n}\nexports.floatx = floatx;\nvar _uidPrefixes = {};\nfunction getUid(prefix) {\n    if (prefix === void 0) { prefix = ''; }\n    if (!(prefix in _uidPrefixes)) {\n        _uidPrefixes[prefix] = 0;\n    }\n    _uidPrefixes[prefix] += 1;\n    return prefix + _uidPrefixes[prefix].toString();\n}\nexports.getUid = getUid;\nfunction softmax(x, axis) {\n    if (axis === void 0) { axis = -1; }\n    return tfc.softmax(x, axis);\n}\nexports.softmax = softmax;\nfunction categoricalCrossentropy(target, output, fromLogits) {\n    if (fromLogits === void 0) { fromLogits = false; }\n    if (fromLogits) {\n        output = softmax(output);\n    }\n    else {\n        var outputSum = sum(output, shape(output).length - 1, true);\n        output = divide(output, outputSum);\n    }\n    output = clip(output, exports.epsilon(), 1 - exports.epsilon());\n    return tfc.neg(tfc.sum(tfc.mul(target, tfc.log(output)), shape(output).length - 1));\n}\nexports.categoricalCrossentropy = categoricalCrossentropy;\nfunction sparseCategoricalCrossentropy(target, output, fromLogits) {\n    if (fromLogits === void 0) { fromLogits = false; }\n    var flatTarget = tfc.floor(flatten(target));\n    var outputShape = shape(output);\n    var oneHotTarget = reshape(tfc.oneHot(flatTarget, outputShape[outputShape.length - 1]), outputShape);\n    return categoricalCrossentropy(oneHotTarget, output, fromLogits);\n}\nexports.sparseCategoricalCrossentropy = sparseCategoricalCrossentropy;\nfunction binaryCrossentropy(target, output, fromLogits) {\n    if (fromLogits === void 0) { fromLogits = false; }\n    var y;\n    if (!fromLogits) {\n        y = clip(output, exports.epsilon(), 1 - exports.epsilon());\n        y = log(divide(y, subtract(tfc.onesLike(y), y)));\n    }\n    else {\n        y = output;\n    }\n    return sigmoidCrossEntropyWithLogits(target, y);\n}\nexports.binaryCrossentropy = binaryCrossentropy;\nfunction sigmoidCrossEntropyWithLogits(target, output) {\n    var maxOutput = tfc.maximum(output, tfc.zerosLike(output));\n    var outputXTarget = tfc.mul(output, target);\n    var sigmoidOutput = tfc.log(tfc.add(getScalar(1), tfc.exp(tfc.neg(tfc.abs(output)))));\n    var result = tfc.add(tfc.sub(maxOutput, outputXTarget), sigmoidOutput);\n    return result;\n}\nexports.sigmoidCrossEntropyWithLogits = sigmoidCrossEntropyWithLogits;\nfunction sigmoid(x) {\n    return tfc.sigmoid(x);\n}\nexports.sigmoid = sigmoid;\nfunction hardSigmoid(x) {\n    var y = scalarPlusArray(tfjs_core_1.scalar(0.5), scalarTimesArray(tfjs_core_1.scalar(0.2), x));\n    return clip(y, 0, 1);\n}\nexports.hardSigmoid = hardSigmoid;\nfunction inTrainPhase(x, alt, training) {\n    if (training === void 0) { training = false; }\n    return training ? x() : alt();\n}\nexports.inTrainPhase = inTrainPhase;\nfunction rnn(stepFunction, inputs, initialStates, goBackwards, mask, constants, unroll, inputLength) {\n    if (goBackwards === void 0) { goBackwards = false; }\n    if (unroll === void 0) { unroll = false; }\n    var ndim = inputs.shape.length;\n    if (ndim < 3) {\n        throw new errors_1.ValueError(\"Input should be at least 3D, but is \" + ndim + \"D.\");\n    }\n    var axes = [1, 0].concat(_.range(2, ndim));\n    inputs = transpose(inputs, axes);\n    if (mask != null) {\n        throw new errors_1.NotImplementedError('The rnn() function of the deeplearn.js backend does not support ' +\n            'masking yet.');\n    }\n    if (constants != null) {\n        throw new errors_1.NotImplementedError('The rnn() functoin of the deeplearn.js backend does not support ' +\n            'constants yet.');\n    }\n    if (unroll) {\n        console.warn('Backend rnn(): the unroll = true option is not applicable to the ' +\n            'imperative deeplearn.js backend.');\n    }\n    if (goBackwards) {\n        inputs = reverse(inputs, 0);\n    }\n    var outputs;\n    var lastOutput;\n    var states = initialStates;\n    var timeSteps = inputs.shape[0];\n    for (var t = 0; t < timeSteps; ++t) {\n        var currentInput = sliceAlongFirstAxis(inputs, t, 1);\n        currentInput = reshape(currentInput, currentInput.shape.slice(1));\n        var stepOutputs = stepFunction(currentInput, states);\n        lastOutput = stepOutputs[0];\n        if (t === 0) {\n            outputs = lastOutput.reshape([1].concat(lastOutput.shape));\n        }\n        else {\n            outputs = concatAlongFirstAxis(outputs, lastOutput.reshape([1].concat(lastOutput.shape)));\n        }\n        states = stepOutputs[1];\n    }\n    return [\n        lastOutput,\n        transpose(outputs, [1, 0].concat(_.range(2, outputs.shape.length))), states\n    ];\n}\nexports.rnn = rnn;\nfunction gradients(lossFn, variables) {\n    var variableList = variables.map(function (variable) { return variable.read(); });\n    var valudAndGrads = tfjs_core_1.variableGrads(lossFn, variableList);\n    return variables.map(function (variable) { return valudAndGrads.grads[variable.name]; });\n}\nexports.gradients = gradients;\n"},"hash":"798dc077486022db23b4faa6ab435ead","cacheData":{"env":{}}}