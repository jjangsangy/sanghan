{"dependencies":[{"name":"/Users/sanghan/Desktop/hotdog/package.json","includedInParent":true,"mtime":1522646366212},{"name":"/Users/sanghan/Desktop/hotdog/node_modules/@tensorflow/tfjs-layers/package.json","includedInParent":true,"mtime":1522260134000},{"name":"underscore","loc":{"line":13,"column":16}},{"name":"../backend/tfjs_backend","loc":{"line":14,"column":16}},{"name":"../constraints","loc":{"line":15,"column":28}},{"name":"../engine/topology","loc":{"line":16,"column":25}},{"name":"../errors","loc":{"line":17,"column":23}},{"name":"../initializers","loc":{"line":18,"column":29}},{"name":"../regularizers","loc":{"line":19,"column":29}},{"name":"../utils/generic_utils","loc":{"line":20,"column":28}}],"generated":{"js":"\"use strict\";\nvar __extends = (this && this.__extends) || (function () {\n    var extendStatics = Object.setPrototypeOf ||\n        ({ __proto__: [] } instanceof Array && function (d, b) { d.__proto__ = b; }) ||\n        function (d, b) { for (var p in b) if (b.hasOwnProperty(p)) d[p] = b[p]; };\n    return function (d, b) {\n        extendStatics(d, b);\n        function __() { this.constructor = d; }\n        d.prototype = b === null ? Object.create(b) : (__.prototype = b.prototype, new __());\n    };\n})();\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar _ = require(\"underscore\");\nvar K = require(\"../backend/tfjs_backend\");\nvar constraints_1 = require(\"../constraints\");\nvar topology_1 = require(\"../engine/topology\");\nvar errors_1 = require(\"../errors\");\nvar initializers_1 = require(\"../initializers\");\nvar regularizers_1 = require(\"../regularizers\");\nvar generic_utils = require(\"../utils/generic_utils\");\nvar BatchNormalization = (function (_super) {\n    __extends(BatchNormalization, _super);\n    function BatchNormalization(config) {\n        var _this = _super.call(this, config) || this;\n        _this.supportsMasking = true;\n        _this.axis = config.axis == null ? -1 : config.axis;\n        _this.momentum = config.momentum == null ? 0.99 : config.momentum;\n        _this.epsilon = config.epsilon == null ? 1e-3 : config.epsilon;\n        _this.center = config.center == null ? true : config.center;\n        _this.scale = config.scale == null ? true : config.scale;\n        _this.betaInitializer = initializers_1.getInitializer(config.betaInitializer || 'zeros');\n        _this.gammaInitializer = initializers_1.getInitializer(config.gammaInitializer || 'ones');\n        _this.movingMeanInitializer =\n            initializers_1.getInitializer(config.movingMeanInitializer || 'zeros');\n        _this.movingVarianceInitializer =\n            initializers_1.getInitializer(config.movingVarianceInitializer || 'ones');\n        _this.betaConstraint = constraints_1.getConstraint(config.betaConstraint);\n        _this.gammaConstraint = constraints_1.getConstraint(config.gammaConstraint);\n        _this.betaRegularizer = regularizers_1.getRegularizer(config.betaRegularizer);\n        _this.gammaRegularizer = regularizers_1.getRegularizer(config.gammaRegularizer);\n        return _this;\n    }\n    BatchNormalization.prototype.build = function (inputShape) {\n        inputShape = generic_utils.getExactlyOneShape(inputShape);\n        var axis = this.axis >= 0 ? this.axis : (this.axis + inputShape.length);\n        var dim = inputShape[axis];\n        if (dim == null) {\n            throw new errors_1.ValueError(\"Axis \" + axis + \" of input tensor should have a defined dimension but \" +\n                \"the layer received an input with shape \" +\n                (JSON.stringify(inputShape) + \".\"));\n        }\n        this.inputSpec =\n            [new topology_1.InputSpec({ ndim: inputShape.length, axes: (_a = {}, _a[axis] = dim, _a) })];\n        var shape = [dim];\n        if (this.scale) {\n            this.gamma = this.addWeight('gamma', shape, null, this.gammaInitializer, this.gammaRegularizer, true, this.gammaConstraint);\n        }\n        if (this.center) {\n            this.beta = this.addWeight('beta', shape, null, this.betaInitializer, this.betaRegularizer, true, this.betaConstraint);\n        }\n        this.movingMean = this.addWeight('moving_mean', shape, null, this.movingMeanInitializer, null, false);\n        this.movingVariance = this.addWeight('moving_variance', shape, null, this.movingVarianceInitializer, null, false);\n        this.built = true;\n        var _a;\n    };\n    BatchNormalization.prototype.call = function (inputs, kwargs) {\n        var _this = this;\n        var training = kwargs['training'] == null ? false : kwargs['training'];\n        var input = generic_utils.getExactlyOneTensor(inputs);\n        var inputShape = K.shape(input);\n        var ndim = inputShape.length;\n        var reductionAxes = _.range(ndim);\n        var axis = this.axis >= 0 ? this.axis : (this.axis + ndim);\n        reductionAxes.splice(axis, 1);\n        var broadcastShape = generic_utils.pyListRepeat(1, ndim);\n        broadcastShape[axis] = inputShape[axis];\n        var sortedReductionAxes = reductionAxes.slice();\n        sortedReductionAxes.sort();\n        var needsBroadcasting = !_.isEqual(sortedReductionAxes, _.range(ndim).slice(0, ndim - 1));\n        var normalizeInference = function () {\n            if (needsBroadcasting) {\n                var broadcastMovingMean = K.reshape(_this.movingMean.read(), broadcastShape);\n                var broadcastMovingVariance = K.reshape(_this.movingVariance.read(), broadcastShape);\n                var broadcastBeta = _this.center ? K.reshape(_this.beta.read(), broadcastShape) : null;\n                var broadcastGamma = _this.center ? K.reshape(_this.gamma.read(), broadcastShape) : null;\n                return K.batchNormalization(input, broadcastMovingMean, broadcastMovingVariance, broadcastBeta, broadcastGamma, _this.epsilon);\n            }\n            else {\n                return K.batchNormalization(input, _this.movingMean.read(), _this.movingVariance.read(), _this.beta.read(), _this.gamma.read(), _this.epsilon);\n            }\n        };\n        if (!training) {\n            return normalizeInference();\n        }\n        throw new errors_1.NotImplementedError('BatchNormalization.call() has not been implemented for training ' +\n            'mode yet.');\n    };\n    BatchNormalization.prototype.getConfig = function () {\n        var config = {\n            axis: this.axis,\n            momentum: this.momentum,\n            epsilon: this.epsilon,\n            center: this.center,\n            scale: this.scale,\n            betaInitializer: initializers_1.serializeInitializer(this.betaInitializer),\n            gammaInitializer: initializers_1.serializeInitializer(this.gammaInitializer),\n            movingMeanInitializer: initializers_1.serializeInitializer(this.movingMeanInitializer),\n            movingVarianceInitializer: initializers_1.serializeInitializer(this.movingVarianceInitializer),\n            betaRegularizer: regularizers_1.serializeRegularizer(this.betaRegularizer),\n            gammaRegularizer: regularizers_1.serializeRegularizer(this.gammaRegularizer),\n            betaConstraint: constraints_1.serializeConstraint(this.betaConstraint),\n            gammaConstraint: constraints_1.serializeConstraint(this.gammaConstraint)\n        };\n        var baseConfig = _super.prototype.getConfig.call(this);\n        Object.assign(config, baseConfig);\n        return config;\n    };\n    return BatchNormalization;\n}(topology_1.Layer));\nexports.BatchNormalization = BatchNormalization;\ngeneric_utils.ClassNameMap.register('BatchNormalization', BatchNormalization);\n"},"hash":"1804055a9cfc8a2832e08decffeb0cf3","cacheData":{"env":{}}}