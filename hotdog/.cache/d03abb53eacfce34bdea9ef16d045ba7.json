{"dependencies":[{"name":"/Users/sanghan/Desktop/hotdog/package.json","includedInParent":true,"mtime":1522646366212},{"name":"/Users/sanghan/Desktop/hotdog/node_modules/@tensorflow/tfjs-layers/package.json","includedInParent":true,"mtime":1522260134000},{"name":"@tensorflow/tfjs-core","loc":{"line":3,"column":26}},{"name":"./backend/tfjs_backend","loc":{"line":4,"column":16}},{"name":"./errors","loc":{"line":5,"column":23}}],"generated":{"js":"\"use strict\";\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar tfjs_core_1 = require(\"@tensorflow/tfjs-core\");\nvar K = require(\"./backend/tfjs_backend\");\nvar errors_1 = require(\"./errors\");\nfunction getActivation(activationType) {\n    if (activationType == null) {\n        return linear;\n    }\n    else if (activationType.toLowerCase() === 'elu') {\n        return elu;\n    }\n    else if (activationType.toLowerCase() === 'hardsigmoid') {\n        return hardSigmoid;\n    }\n    else if (activationType.toLowerCase() === 'linear') {\n        return linear;\n    }\n    else if (activationType.toLowerCase() === 'relu') {\n        return relu;\n    }\n    else if (activationType.toLowerCase() === 'relu6') {\n        return relu6;\n    }\n    else if (activationType.toLowerCase() === 'selu') {\n        return selu;\n    }\n    else if (activationType.toLowerCase() === 'sigmoid') {\n        return sigmoid;\n    }\n    else if (activationType.toLowerCase() === 'softmax') {\n        return softmax;\n    }\n    else if (activationType.toLowerCase() === 'softplus') {\n        return softplus;\n    }\n    else if (activationType.toLowerCase() === 'softsign') {\n        return softsign;\n    }\n    else if (activationType.toLowerCase() === 'tanh') {\n        return tanh;\n    }\n    else {\n        throw new errors_1.ValueError(\"Unsupported activation function \" + activationType);\n    }\n}\nexports.getActivation = getActivation;\nfunction elu(x, alpha) {\n    if (alpha === void 0) { alpha = 1; }\n    return K.elu(x, alpha);\n}\nexports.elu = elu;\nfunction selu(x) {\n    return K.selu(x);\n}\nexports.selu = selu;\nfunction relu(x) {\n    return K.relu(x);\n}\nexports.relu = relu;\nfunction relu6(x) {\n    return K.minimum(tfjs_core_1.scalar(6.0), K.relu(x));\n}\nexports.relu6 = relu6;\nfunction linear(x) {\n    return x;\n}\nexports.linear = linear;\nfunction sigmoid(x) {\n    return K.sigmoid(x);\n}\nexports.sigmoid = sigmoid;\nfunction hardSigmoid(x) {\n    return K.hardSigmoid(x);\n}\nexports.hardSigmoid = hardSigmoid;\nfunction softplus(x) {\n    return K.softplus(x);\n}\nexports.softplus = softplus;\nfunction softsign(x) {\n    return K.softsign(x);\n}\nexports.softsign = softsign;\nfunction tanh(x) {\n    return K.tanh(x);\n}\nexports.tanh = tanh;\nfunction softmax(x, axis) {\n    if (axis === void 0) { axis = (-1); }\n    return K.softmax(x, axis);\n}\nexports.softmax = softmax;\nfunction serializeActivation(activation) {\n    return activation.name;\n}\nexports.serializeActivation = serializeActivation;\n"},"hash":"ed1c957967b2c52a4a8f1ce5352e934e","cacheData":{"env":{}}}