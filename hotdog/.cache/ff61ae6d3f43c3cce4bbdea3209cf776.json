{"dependencies":[{"name":"/Users/sanghan/Desktop/hotdog/package.json","includedInParent":true,"mtime":1522646366212},{"name":"/Users/sanghan/Desktop/hotdog/node_modules/@tensorflow/tfjs-core/package.json","includedInParent":true,"mtime":0},{"name":"../environment","loc":{"line":13,"column":28}},{"name":"../globals","loc":{"line":14,"column":24}},{"name":"../ops/ops","loc":{"line":15,"column":20}},{"name":"./optimizer","loc":{"line":16,"column":26}}],"generated":{"js":"\"use strict\";\nvar __extends = (this && this.__extends) || (function () {\n    var extendStatics = Object.setPrototypeOf ||\n        ({ __proto__: [] } instanceof Array && function (d, b) { d.__proto__ = b; }) ||\n        function (d, b) { for (var p in b) if (b.hasOwnProperty(p)) d[p] = b[p]; };\n    return function (d, b) {\n        extendStatics(d, b);\n        function __() { this.constructor = d; }\n        d.prototype = b === null ? Object.create(b) : (__.prototype = b.prototype, new __());\n    };\n})();\nObject.defineProperty(exports, \"__esModule\", { value: true });\nvar environment_1 = require(\"../environment\");\nvar globals_1 = require(\"../globals\");\nvar ops_1 = require(\"../ops/ops\");\nvar optimizer_1 = require(\"./optimizer\");\nvar AdadeltaOptimizer = (function (_super) {\n    __extends(AdadeltaOptimizer, _super);\n    function AdadeltaOptimizer(learningRate, rho, epsilon) {\n        if (epsilon === void 0) { epsilon = 1e-8; }\n        var _this = _super.call(this) || this;\n        _this.accumulatedGrads = {};\n        _this.accumulatedUpdates = {};\n        _this.c = globals_1.keep(ops_1.scalar(-learningRate));\n        _this.epsilon = globals_1.keep(ops_1.scalar(epsilon));\n        _this.rho = globals_1.keep(ops_1.scalar(rho));\n        _this.oneMinusRho = globals_1.keep(ops_1.scalar(1 - rho));\n        return _this;\n    }\n    AdadeltaOptimizer.prototype.applyGradients = function (variableGradients) {\n        var _this = this;\n        var _loop_1 = function (variableName) {\n            var value = environment_1.ENV.engine.registeredVariables[variableName];\n            if (this_1.accumulatedGrads[variableName] == null) {\n                var trainable_1 = false;\n                globals_1.tidy(function () {\n                    _this.accumulatedGrads[variableName] =\n                        ops_1.zerosLike(value).variable(trainable_1);\n                });\n            }\n            if (this_1.accumulatedUpdates[variableName] == null) {\n                var trainable_2 = false;\n                globals_1.tidy(function () {\n                    _this.accumulatedUpdates[variableName] =\n                        ops_1.zerosLike(value).variable(trainable_2);\n                });\n            }\n            var gradient = variableGradients[variableName];\n            var accumulatedGrad = this_1.accumulatedGrads[variableName];\n            var accumulatedUpdate = this_1.accumulatedUpdates[variableName];\n            globals_1.tidy(function () {\n                var newAccumulatedGrad = _this.rho.mul(accumulatedGrad)\n                    .add(_this.oneMinusRho.mul(gradient.square()));\n                var updates = accumulatedUpdate.add(_this.epsilon)\n                    .sqrt()\n                    .div(accumulatedGrad.add(_this.epsilon).sqrt())\n                    .mul(gradient);\n                var newAccumulatedUpdate = _this.rho.mul(accumulatedUpdate)\n                    .add(_this.oneMinusRho.mul(updates.square()));\n                _this.accumulatedGrads[variableName].assign(newAccumulatedGrad);\n                _this.accumulatedUpdates[variableName].assign(newAccumulatedUpdate);\n                var newValue = _this.c.mul(updates).add(value);\n                value.assign(newValue);\n            });\n        };\n        var this_1 = this;\n        for (var variableName in variableGradients) {\n            _loop_1(variableName);\n        }\n    };\n    AdadeltaOptimizer.prototype.dispose = function () {\n        var _this = this;\n        this.c.dispose();\n        this.epsilon.dispose();\n        this.rho.dispose();\n        this.oneMinusRho.dispose();\n        if (this.accumulatedUpdates != null) {\n            Object.keys(this.accumulatedUpdates)\n                .forEach(function (name) { return _this.accumulatedUpdates[name].dispose(); });\n            Object.keys(this.accumulatedGrads)\n                .forEach(function (name) { return _this.accumulatedGrads[name].dispose(); });\n        }\n    };\n    return AdadeltaOptimizer;\n}(optimizer_1.Optimizer));\nexports.AdadeltaOptimizer = AdadeltaOptimizer;\n"},"hash":"65a7e66f93a96d382c626263307b5c21","cacheData":{"env":{}}}